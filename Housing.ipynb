{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8q7f3jq-hhj"
      },
      "source": [
        "# Configuración Inicial"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHQChiio-781"
      },
      "source": [
        "## Habilitar GPU (solo para Google Colab)\n",
        "\n",
        "En la barra superior:\n",
        "\n",
        "1.   Entorno de ejecución\n",
        "2.   Cambiar tipo de entorno de ejecución -> GPU\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WP-0WpH_Q9l"
      },
      "source": [
        "## Subir *dataset* (solo para Google Colab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "cYjioi4v_Upi",
        "outputId": "bfa36299-448f-42f1-fb33-bb7a0e6cdb4b"
      },
      "outputs": [],
      "source": [
        "### QUITAR COMENTARIOS Y EJECUTAR PARA USAR EN GOOGLE COLAB ###\n",
        "#from google.colab import files\n",
        "\n",
        "#uploaded = files.upload()  # Seleccionar el archivo desde el sistema local"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3CW2Qz_9LVyp"
      },
      "source": [
        "# 1 Procesamiento de datos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.1 Carga y exploración inicial"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.1.1 Carga del dataset y visualización inicial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Cargar el dataset\n",
        "df = pd.read_csv(\"Housing.csv\")  # Asegurarse de tener el archivo en el directorio local, o subirlo manualmente a Google Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualizar primeras filas\n",
        "print(\"Primeras 5 filas:\")\n",
        "print(df.head())\n",
        "\n",
        "# Verificar valores nulos\n",
        "print(\"\\nValores faltantes por columna:\")\n",
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "No hay valores faltantes en ninguna columna. Por tanto, no es necesario hacer tratamiento de valores nulos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.1.2 Análisis de distribuciones y outliers\n",
        "\n",
        "Graficar histogramas y diagramas de caja:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configurar estilo de gráficos\n",
        "sns.set_theme(style=\"whitegrid\")\n",
        "\n",
        "# Histogramas para variables numéricas\n",
        "numerical_cols = ['area', 'price', 'bedrooms', 'bathrooms', 'stories', 'parking']\n",
        "plt.figure(figsize=(15, 10))\n",
        "for i, col in enumerate(numerical_cols, 1):\n",
        "    plt.subplot(2, 3, i)\n",
        "    sns.histplot(df[col], kde=True, bins=30)\n",
        "    plt.title(f'Distribución de {col}')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Boxplots para detectar outliers\n",
        "plt.figure(figsize=(15, 8))\n",
        "for i, col in enumerate(numerical_cols, 1):\n",
        "    plt.subplot(2, 3, i)\n",
        "    sns.boxplot(y=df[col])\n",
        "    plt.title(f'Boxplot de {col}')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- **Histogramas**:\n",
        "\n",
        "    Muestran la distribución de cada variable numérica.\n",
        "\n",
        "    Si la distribución tiene una cola larga a la derecha (**sesgo positivo**), significa que hay valores extremadamente altos. En este caso, sucede con `area` y `price`.\n",
        "\n",
        "- **Boxplots**:\n",
        "\n",
        "    Identifican valores **atípicos** (_outliers_) en cada variable.\n",
        "\n",
        "    Los puntos fuera de los \"bigotes\" (líneas horizontales) son _outliers_. De nuevo, `area` y `price` tienen varios, lo que sugiere que hay propiedades con áreas mucho más grandes que el resto, y con un precio también mucho mayor a los demás."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.2 Preprocesamiento de datos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.2.1 Codificación de variables categóricas\n",
        "\n",
        "- **Binarias**: `1` para \"_yes_\", `0` para \"_no_\".\n",
        "\n",
        "- ¿Por qué es mejor One-Hot Encoding que Label Encoding en este caso?\n",
        "\n",
        "    1. Evita jerarquías artificiales:\n",
        "\n",
        "        La red neuronal podría interpretar erróneamente que unfurnished (0) < semi-furnished (1) < furnished (2), lo que no necesariamente refleja la realidad en los precios.\n",
        "\n",
        "    2. Flexibilidad del modelo:\n",
        "\n",
        "        Con One-Hot, cada categoría se trata como una característica independiente, permitiendo al modelo aprender contribuciones no lineales.\n",
        "\n",
        "    3. Ejemplo práctico:\n",
        "\n",
        "        Una propiedad semi-furnished no es el \"punto medio\" entre unfurnished y furnished en términos de precio. One-Hot captura mejor esta relación."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Variables binarias (yes/no)\n",
        "binary_cols = ['mainroad', 'guestroom', 'basement', 'hotwaterheating', 'airconditioning', 'prefarea']\n",
        "df[binary_cols] = df[binary_cols].replace({'yes': 1, 'no': 0})\n",
        "\n",
        "# One-Hot Encoding para furnishingstatus\n",
        "df = pd.get_dummies(df, columns=['furnishingstatus'], prefix='furnishing', dtype=int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verificar cambios\n",
        "df.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.2.2 Normalización de Variables Numéricas\n",
        "\n",
        "`StandardScaler` centra las variables en 0 con **desviación estándar** 1. De esta forma, evita que variables como `area` (valores grandes) dominen el modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Separar variables numéricas\n",
        "numerical_cols = ['area', 'bedrooms', 'bathrooms', 'stories', 'parking']\n",
        "X_numerical = df[numerical_cols]\n",
        "\n",
        "# Escalado (usando StandardScaler)\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X_numerical)\n",
        "\n",
        "# Reemplazar columnas originales con valores escalados\n",
        "df[numerical_cols] = X_scaled"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verificar cambios\n",
        "df[numerical_cols].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.3 Análisis de correlación"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.3.1 Matriz de correlación"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Calcular matriz de correlación\n",
        "corr_matrix = df.corr()\n",
        "\n",
        "# Matriz de correlación para todas las variables\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
        "plt.title(\"Matriz de Correlación\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "También se puede evaluar sólo la columna de la **matriz de correlación** en función de `price`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Filtrar solo las correlaciones con 'price'\n",
        "price_corr = corr_matrix[['price']].sort_values(by='price', ascending=False)\n",
        "\n",
        "# Gráfico de correlaciones con 'price'\n",
        "plt.figure(figsize=(8, 10))\n",
        "sns.heatmap(price_corr, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
        "plt.title(\"Correlación de Variables con el Precio (price)\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Variables Fuertemente Correlacionadas con `price`:\n",
        "\n",
        "- Alta correlación **positiva** (`> 0.4`): `area`, `bathrooms`, `airconditioning`: A mayor área, número de baños y/o con aire acondicionado, mayor precio.\n",
        "\n",
        "- Correlación **negativa** (`< -0.3`): `furnishing_unfurnished`: Las propiedades no amuebladas tienden a ser más baratas.\n",
        "\n",
        "- **Poca** o casi ninguna correlación:\n",
        "\n",
        "    - `hotwaterheating` (`0.093`): Saber si una casa tiene o no agua caliente, aporta muy poca información sobre el precio de la misma.\n",
        "\n",
        "    - `furnishing_semi-furnished` (`0.064`): Que una casa esté \"semiamueblada\" tiene un impacto mínimo en el precio."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.3.2 Eliminación de variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Con la información que se ha obtenido al realizar el análisis anterior, se pueden sacar las siguientes **conclusiones**:\n",
        "\n",
        "- `hotwaterheating`: Se puede **eliminar** esta columna, ya que su correlación con `price` es muy baja, y no aporta información significativa al modelo.\n",
        "\n",
        "- `furnishing_semi-furnished`: **No** se va a eliminar esta columna, ya que podría carecer de lógica que una vivienda tuviera valor 0 en las otras dos categorías de `furnishing`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Eliminar la columna hotwaterheating\n",
        "df.drop('hotwaterheating', axis=1, inplace=True)\n",
        "\n",
        "# Verificar el dataset actualizado\n",
        "df.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.4 División Train-Test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Creación de datos sintéticos (Data Augmentation):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Separar características (X) y variable objetivo (y)\n",
        "X = df.drop('price', axis=1)  # Todas las columnas excepto 'price'\n",
        "y = df['price']  # Columna 'price' como variable objetivo\n",
        "\n",
        "def augment_data(X, y, num_samples=350):\n",
        "    np.random.seed(42)\n",
        "    \n",
        "    # Interpolación entre puntos reales\n",
        "    idx1 = np.random.randint(0, len(X), num_samples)\n",
        "    idx2 = np.random.randint(0, len(X), num_samples)\n",
        "    \n",
        "    alpha = np.random.uniform(0, 1, num_samples).reshape(-1, 1)\n",
        "    X_aug = alpha * X.iloc[idx1].values + (1 - alpha) * X.iloc[idx2].values\n",
        "    y_aug = alpha.flatten() * y.iloc[idx1].values + (1 - alpha.flatten()) * y.iloc[idx2].values\n",
        "    \n",
        "    # Agregar ruido gaussiano\n",
        "    noise_X = np.random.normal(0, 0.01, X_aug.shape)\n",
        "    noise_y = np.random.normal(0, 0.01, y_aug.shape)\n",
        "    \n",
        "    X_aug += noise_X\n",
        "    y_aug += noise_y\n",
        "    \n",
        "    X_aug = pd.DataFrame(X_aug, columns=X.columns)\n",
        "    y_aug = pd.Series(y_aug, name=y.name)\n",
        "    \n",
        "    return X_aug, y_aug\n",
        "\n",
        "# Aplicar aumentación\n",
        "X_aug, y_aug = augment_data(X, y, num_samples=500)\n",
        "X = pd.concat([X, X_aug], axis=0)\n",
        "y = pd.concat([y, y_aug], axis=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Se utiliza `train_test_split` para dividir el conjunto de datos en 80% datos de entrenamiento y 20% datos de prueba."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# División: 80% entrenamiento, 20% prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verificar tamaños de los conjuntos\n",
        "print(f\"Tamaño del conjunto de entrenamiento: {X_train.shape}\")\n",
        "print(f\"Tamaño del conjunto de prueba: {X_test.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train.head(), y_train.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2 Model Planning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Asegurarse de que `TensorFlow` esté usando la GPU:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "print(\"GPUs disponibles:\", tf.config.list_physical_devices('GPU'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.1 Construcción del modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.initializers import HeNormal\n",
        "\n",
        "# Función para crear el modelo con opciones de regularización y Dropout\n",
        "def create_model(optimizer, dropout_rate=0.0, l2_reg=0.0, n_units=64):\n",
        "    model = Sequential([\n",
        "        Dense(n_units, activation='relu', input_shape=(X_train.shape[1],),\n",
        "              kernel_initializer=HeNormal(), kernel_regularizer=l2(l2_reg)),  # He normal initialization\n",
        "        BatchNormalization(),\n",
        "        Dropout(dropout_rate),  # Dropout para evitar sobreajuste\n",
        "        Dense(n_units, activation='relu', kernel_initializer=HeNormal(), kernel_regularizer=l2(l2_reg)),\n",
        "        BatchNormalization(),\n",
        "        Dropout(dropout_rate),\n",
        "        Dense(n_units // 2, activation='relu', kernel_initializer=HeNormal(), kernel_regularizer=l2(l2_reg)),\n",
        "        Dense(1, activation='linear')  # Capa de salida\n",
        "    ])\n",
        "    model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.2 Comparativa de optimizadores y validación cruzada"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Input, Dropout\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "# Configuración de K-Fold\n",
        "k = 5\n",
        "kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
        "\n",
        "# Diccionarios para almacenar métricas\n",
        "metrics_adam = {'mae': [], 'rmse': [], 'r2': []}\n",
        "metrics_sgd  = {'mae': [], 'rmse': [], 'r2': []}\n",
        "\n",
        "# Iterar sobre cada partición del K-Fold\n",
        "for train_index, val_index in kf.split(X_train):\n",
        "    X_train_fold = X_train.iloc[train_index]\n",
        "    y_train_fold = y_train.iloc[train_index]\n",
        "    X_val_fold = X_train.iloc[val_index]\n",
        "    y_val_fold = y_train.iloc[val_index]\n",
        "\n",
        "    # --- Modelo con Adam y Gradient Clipping ---\n",
        "    model_adam = create_model(Adam(learning_rate=0.001, clipvalue=0.5))\n",
        "    model_adam.fit(\n",
        "        X_train_fold, \n",
        "        y_train_fold, \n",
        "        validation_data=(X_val_fold, y_val_fold),\n",
        "        epochs=100, \n",
        "        batch_size=32, \n",
        "        verbose=0\n",
        "    )\n",
        "    y_pred_adam = model_adam.predict(X_val_fold).flatten()\n",
        "\n",
        "    # Cálculo de métricas para Adam\n",
        "    metrics_adam['mae'].append(mean_absolute_error(y_val_fold, y_pred_adam))\n",
        "    metrics_adam['rmse'].append(np.sqrt(mean_squared_error(y_val_fold, y_pred_adam)))\n",
        "    metrics_adam['r2'].append(r2_score(y_val_fold, y_pred_adam))\n",
        "\n",
        "    # --- Modelo con SGD y Gradient Clipping ---\n",
        "    model_sgd = create_model(SGD(learning_rate=0.01, momentum=0.9, clipvalue=0.5))\n",
        "    model_sgd.fit(\n",
        "        X_train_fold,\n",
        "        y_train_fold,\n",
        "        validation_data=(X_val_fold, y_val_fold),\n",
        "        epochs=100,\n",
        "        batch_size=32,\n",
        "        verbose=0\n",
        "    )\n",
        "    y_pred_sgd = model_sgd.predict(X_val_fold).flatten()\n",
        "\n",
        "    # Cálculo de métricas para SGD\n",
        "    metrics_sgd['mae'].append(mean_absolute_error(y_val_fold, y_pred_sgd))\n",
        "    metrics_sgd['rmse'].append(np.sqrt(mean_squared_error(y_val_fold, y_pred_sgd)))\n",
        "    metrics_sgd['r2'].append(r2_score(y_val_fold, y_pred_sgd))\n",
        "\n",
        "# Resultados promedio\n",
        "print(\"Resultados Adam (Cross-Validation):\")\n",
        "print(f\"MAE: {np.mean(metrics_adam['mae']):.4f} ± {np.std(metrics_adam['mae']):.4f}\")\n",
        "print(f\"RMSE: {np.mean(metrics_adam['rmse']):.4f} ± {np.std(metrics_adam['rmse']):.4f}\")\n",
        "print(f\"R²: {np.mean(metrics_adam['r2']):.4f} ± {np.std(metrics_adam['r2']):.4f}\")\n",
        "\n",
        "print(\"\\nResultados SGD (Cross-Validation):\")\n",
        "print(f\"MAE: {np.mean(metrics_sgd['mae']):.4f} ± {np.std(metrics_sgd['mae']):.4f}\")\n",
        "print(f\"RMSE: {np.mean(metrics_sgd['rmse']):.4f} ± {np.std(metrics_sgd['rmse']):.4f}\")\n",
        "print(f\"R²: {np.mean(metrics_sgd['r2']):.4f} ± {np.std(metrics_sgd['r2']):.4f}\")\n",
        "\n",
        "# Evaluación final en test set (opcional)\n",
        "final_model = create_model(Adam(learning_rate=0.0001, clipvalue=1.0))\n",
        "final_model.fit(X_train, y_train, epochs=350, batch_size=32, verbose=0)\n",
        "y_test_pred = final_model.predict(X_test).flatten()\n",
        "\n",
        "print(\"\\nEvaluación final en Test Set:\")\n",
        "print(f\"MAE: {mean_absolute_error(y_test, y_test_pred):.4f}\")\n",
        "print(f\"RMSE: {np.sqrt(mean_squared_error(y_test, y_test_pred)):.4f}\")\n",
        "print(f\"R²: {r2_score(y_test, y_test_pred):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.3 Entrenamiento del modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dividir datos en entrenamiento y validación (80%-20%)\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "# Crear el modelo con el optimizador Adam\n",
        "model_adam = create_model(optimizer=Adam(learning_rate=0.001), dropout_rate=0.3, l2_reg=0.01)\n",
        "\n",
        "# Entrenar el modelo\n",
        "history = model_adam.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=350,\n",
        "    batch_size=32,\n",
        "    verbose=1\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.4 Definición de la arquitectura final del modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Arquitectura del modelo con optimizador como argumento\n",
        "def create_model(optimizer, dropout_rate, l2_reg):\n",
        "    model = Sequential([\n",
        "        Input(shape=(X_train.shape[1],)),                                                               # Capa de entrada (12 features)\n",
        "        Dense(64, activation='relu', kernel_initializer=HeNormal(), kernel_regularizer=l2(l2_reg)),     # Primera capa oculta\n",
        "        BatchNormalization(),                                                                           # Normalización por lotes\n",
        "        Dropout(dropout_rate),                                                                          # Regularización por Dropout\n",
        "        Dense(64, activation='relu', kernel_initializer=HeNormal(), kernel_regularizer=l2(l2_reg)),     # Segunda capa oculta\n",
        "        BatchNormalization(),                                                                           # Ayuda con la estabilidad\n",
        "        Dropout(dropout_rate),                                                                          # Regularización por Dropout\n",
        "        Dense(32, activation='relu', kernel_initializer=HeNormal(), kernel_regularizer=l2(l2_reg)),     # Tercera capa oculta\n",
        "        Dense(1, activation='linear')                                                                   # Capa de salida con activación lineal\n",
        "    ])\n",
        "    model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3 Model building and selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.1 Experimentar con distintos hiperparámetros"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Experimentación con diferentes hiperparámetros\n",
        "def experiment_with_hyperparameters():\n",
        "    results = {}\n",
        "    for lr in [0.01, 0.001, 0.0001]:\n",
        "        for batch_size in [16, 32, 64]:\n",
        "            for dropout in [0.2, 0.3]:\n",
        "                for l2_reg in [0.1, 0.01]:\n",
        "                    print(f\"Entrenando con lr={lr}, batch_size={batch_size}, dropout={dropout}, l2_reg={l2_reg}\")\n",
        "                    model = create_model(optimizer=Adam(learning_rate=lr), dropout_rate=dropout, l2_reg=l2_reg)\n",
        "                    history = model.fit(\n",
        "                        X_train, y_train,\n",
        "                        validation_data=(X_val, y_val),\n",
        "                        epochs=350,\n",
        "                        batch_size=batch_size,\n",
        "                        verbose=0\n",
        "                    )\n",
        "                    val_loss = history.history['val_loss'][-1]\n",
        "                    results[(lr, batch_size, dropout, l2_reg)] = val_loss\n",
        "    return results\n",
        "\n",
        "# Ejecutar experimentos\n",
        "hyperparameter_results = experiment_with_hyperparameters()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Para obtener la mejor combinación de hiperparámetros:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ordenar resultados del experimento por menor pérdida de validación (val_loss)\n",
        "sorted_results = sorted(hyperparameter_results.items(), key=lambda x: x[1])\n",
        "\n",
        "# Mostrar las 5 mejores combinaciones\n",
        "print(\"Top 5 mejores combinaciones de hiperparámetros:\")\n",
        "for params, val_loss in sorted_results[:5]:\n",
        "    print(f\"LR: {params[0]}, Batch Size: {params[1]}, Dropout: {params[2]}, L2: {params[3]} --> Val Loss: {val_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Obtener la mejor combinación de hiperparámetros\n",
        "best_hyperparams = sorted_results[0][0]  # La combinación con menor val_loss\n",
        "\n",
        "# Crear y entrenar el modelo con la mejor combinación\n",
        "best_model = create_model(optimizer=Adam(learning_rate=best_hyperparams[0]), \n",
        "                          dropout_rate=best_hyperparams[2], \n",
        "                          l2_reg=best_hyperparams[3])\n",
        "\n",
        "history = best_model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=400,\n",
        "    batch_size=best_hyperparams[1],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Mostrar métricas finales en entrenamiento y validación\n",
        "final_train_loss = history.history['loss'][-1]\n",
        "final_val_loss = history.history['val_loss'][-1]\n",
        "\n",
        "print(f\"\\nMejor combinación de hiperparámetros:\")\n",
        "print(f\"LR: {best_hyperparams[0]}, Batch Size: {best_hyperparams[1]}, Dropout: {best_hyperparams[2]}, L2: {best_hyperparams[3]}\")\n",
        "print(f\"Train Loss: {final_train_loss:.4f}, Val Loss: {final_val_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.2 Calcular porcentaje de error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calcular RMSE\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
        "print(f\"RMSE: {rmse:.4f}\")\n",
        "\n",
        "# RMSE como porcentaje del rango de los valores reales\n",
        "rmse_range_percentage = (rmse / (np.max(y_test) - np.min(y_test))) * 100\n",
        "\n",
        "print(f\"RMSE (% sobre rango): {rmse_range_percentage:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 4 Presentación de resultados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4.1 Evaluación de predicciones"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.1.1 Gráfico de dispersión entre valores reales y predicciones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Predicciones del modelo\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Gráfico de dispersión\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.scatter(y_test, y_pred, alpha=0.5, label=\"Predicciones vs Reales\")\n",
        "plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], '--r', label=\"Línea Ideal (y=x)\")\n",
        "plt.xlabel(\"Valores Reales\")\n",
        "plt.ylabel(\"Predicciones\")\n",
        "plt.title(\"Gráfico de dispersión: Predicciones vs Valores Reales\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Los puntos están generalmente alineados y cercanos a la línea roja (línea ideal), lo que indica que esas predicciones son precisas.\n",
        "\n",
        "- Las predicciones que se encuentran muy dispersas respecto a la línea, pueden indicar un sesgo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.1.2 Gráfico de residuales"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "residuals = y_test - y_pred.flatten()\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.scatter(y_pred, residuals, alpha=0.5)\n",
        "plt.axhline(0, color='red', linestyle='--')\n",
        "plt.xlabel(\"Predicciones\")\n",
        "plt.ylabel(\"Residuales\")\n",
        "plt.title(\"Gráfico de residuales\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Los puntos están dispersos aleatoriamente, lo que indica que el modelo está bien ajustado.\n",
        "\n",
        "- Si hubiera una tendencia clara (por ejemplo, un patrón en forma de U), el modelo podría necesitar ajustes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.1.3 Boxplot de errores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(6,4))\n",
        "plt.boxplot(residuals, vert=False, patch_artist=True)\n",
        "plt.xlabel(\"Error de Predicción\")\n",
        "plt.title(\"Boxplot de Errores\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Los puntos que se encuentran muy alejados, podrían ser _outliers_ en los errores."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.1.4 Curva de aprendizaje"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8,6))\n",
        "plt.plot(history.history['loss'], label='Pérdida en Entrenamiento')\n",
        "plt.plot(history.history['val_loss'], label='Pérdida en Validación')\n",
        "plt.xlabel(\"Épocas\")\n",
        "plt.ylabel(\"Pérdida\")\n",
        "plt.title(\"Curva de Aprendizaje\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Si la pérdida de validación es mucho mayor que la de entrenamiento → Sobreajuste.\n",
        "\n",
        "- Si ambas pérdidas son altas → El modelo podría ser demasiado simple."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4.2 Análisis de errores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.2.1 Predicciones con errores mayores al 10%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Asegurar que y_test y y_pred sean 1D\n",
        "y_test = np.ravel(y_test)  # Convierte a 1D si es necesario\n",
        "y_pred = np.ravel(y_pred)  # Convierte a 1D si es necesario\n",
        "\n",
        "# Calcular el error relativo\n",
        "errores_relativos = np.abs((y_test - y_pred) / y_test)\n",
        "\n",
        "# Filtrar los casos con errores > 10%\n",
        "errores_significativos = errores_relativos > 0.10\n",
        "\n",
        "# Mostrar ejemplos de predicciones con alto error\n",
        "errores_df = pd.DataFrame({\n",
        "    \"Real\": y_test,\n",
        "    \"Predicción\": y_pred,\n",
        "    \"Error Relativo (%)\": errores_relativos * 100\n",
        "})\n",
        "\n",
        "errores_df_significativos = errores_df[errores_significativos].sort_values(by=\"Error Relativo (%)\", ascending=False)\n",
        "\n",
        "print(errores_df_significativos.head(10))  # Ver los 10 peores casos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.2.2 Distribución de los errores relativos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8,6))\n",
        "plt.hist(errores_relativos * 100, bins=30, edgecolor=\"black\", alpha=0.7)\n",
        "plt.axvline(x=10, color=\"red\", linestyle=\"--\", label=\"Error del 10%\")\n",
        "plt.xlabel(\"Error Relativo (%)\")\n",
        "plt.ylabel(\"Frecuencia\")\n",
        "plt.title(\"Distribución del Error Relativo en %\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 5 Serialización del modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Guardado en formato Keras (.keras)\n",
        "final_model.save(\"housing_price_model_v1.keras\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Guardado en formato HDF5 (.h5)\n",
        "final_model.save('housing_price_model_v1.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Cargar el modelo keras y revisar su shape\n",
        "loaded_model = tf.keras.models.load_model('housing_price_model_v1.keras')\n",
        "print(loaded_model.summary())"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
